"""
å®Ÿé¨“çµæœã®è¡¨ç¤ºã¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç®¡ç†ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""
from typing import Dict, Any, List
import numpy as np


class ResultFormatter:
    """å®Ÿé¨“çµæœã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¡¨ç¤ºã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def print_experiment_summary(results: Dict[str, Any]) -> None:
        """å˜ä¸€å®Ÿé¨“ã®çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        config = results["config"]
        data_stats = results["data_stats"]
        evaluation = results["evaluation"]
        
        print("\n--- Experiment Summary ---")
        print(f"Number of samples: {config['n_samples']}")
        print(f"Reward type: {config['reward_type']}")
        print(f"Model type: {config['model_type']}")
        print(f"Use undersampling: {config['use_undersampling']}")
        
        if config['use_undersampling']:
            print(f"K-factor: {config['k_factor']}")
        
        print(f"Test set size: {data_stats['test_samples']}")
        print(f"Original positive rate: {data_stats['original_positive_rate']:.4f}")
        print(f"Training positive rate: {data_stats['train_positive_rate']:.4f}")
        print(f"CATE Estimation RMSE: {evaluation['rmse']:.4f}")
        print(f"CATE Estimation Bias: {evaluation['bias']:.4f}")
        
        if 'r2_score' in evaluation:
            print(f"RÂ² Score: {evaluation['r2_score']:.4f}")
    
    @staticmethod
    def print_comparison_results(comparison_results: Dict[str, Any]) -> None:
        """æ¯”è¼ƒå®Ÿé¨“ã®çµæœã‚’è¡¨ç¤º"""
        comparison = comparison_results["comparison"]
        
        print("\n" + "=" * 60)
        print("COMPARISON RESULTS")
        print("=" * 60)
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼
        print(f"{'Metric':<20} {'Baseline':<15} {'Undersampling':<15} {'Improvement':<15}")
        print("-" * 70)
        
        # RMSE
        print(f"{'RMSE':<20} {comparison['baseline_rmse']:<15.4f} "
              f"{comparison['undersampling_rmse']:<15.4f} "
              f"{comparison['rmse_improvement_percent']:+.2f}%")
        
        # Bias
        print(f"{'Bias (abs)':<20} {abs(comparison['baseline_bias']):<15.4f} "
              f"{abs(comparison['undersampling_bias']):<15.4f} "
              f"{comparison['bias_improvement_percent']:+.2f}%")
    
    @staticmethod
    def print_detailed_evaluation(evaluation: Dict[str, float]) -> None:
        """è©³ç´°ãªè©•ä¾¡çµæœã‚’è¡¨ç¤º"""
        print("\n--- Detailed Evaluation ---")
        for metric, value in evaluation.items():
            print(f"{metric.upper()}: {value:.4f}")
    
    @staticmethod
    def print_data_statistics(data_stats: Dict[str, Any]) -> None:
        """ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã‚’è¡¨ç¤º"""
        print("\n--- Data Statistics ---")
        print(f"Total samples: {data_stats['total_samples']}")
        print(f"Training samples: {data_stats['train_samples']}")
        print(f"Test samples: {data_stats['test_samples']}")
        print(f"Original positive rate: {data_stats['original_positive_rate']:.4f}")
        print(f"Training positive rate: {data_stats['train_positive_rate']:.4f}")
    
    @staticmethod
    def print_configuration(config: Dict[str, Any]) -> None:
        """è¨­å®šå†…å®¹ã‚’è¡¨ç¤º"""
        print("\n--- Configuration ---")
        for key, value in config.items():
            if key != 'model_params':
                print(f"{key}: {value}")
        
        if 'model_params' in config and config['model_params']:
            print("Model parameters:")
            for param, value in config['model_params'].items():
                print(f"  {param}: {value}")
    
    @staticmethod
    def create_results_summary(results_list: List[Dict[str, Any]]) -> str:
        """è¤‡æ•°å®Ÿé¨“çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ"""
        if not results_list:
            return "No results to summarize."
        
        summary_lines = []
        summary_lines.append("=" * 80)
        summary_lines.append("EXPERIMENT RESULTS SUMMARY")
        summary_lines.append("=" * 80)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        header = f"{'Experiment':<25} {'Reward':<10} {'Model':<15} {'Undersampling':<12} {'RMSE':<8} {'Bias':<8}"
        summary_lines.append(header)
        summary_lines.append("-" * 80)
        
        # å„å®Ÿé¨“ã®çµæœ
        for i, result in enumerate(results_list):
            config = result["config"]
            evaluation = result["evaluation"]
            
            exp_name = f"Experiment {i+1}"
            reward_type = config['reward_type'][:8]  # çŸ­ç¸®
            model_type = config['model_type'][:13]  # çŸ­ç¸®
            undersampling = "Yes" if config['use_undersampling'] else "No"
            rmse = f"{evaluation['rmse']:.4f}"
            bias = f"{evaluation['bias']:.4f}"
            
            line = f"{exp_name:<25} {reward_type:<10} {model_type:<15} {undersampling:<12} {rmse:<8} {bias:<8}"
            summary_lines.append(line)
        
        summary_lines.append("=" * 80)
        
        return "\n".join(summary_lines)
    
    @staticmethod
    def print_experiment_header(title: str) -> None:
        """å®Ÿé¨“é–‹å§‹æ™‚ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¡¨ç¤º"""
        border = "=" * len(title)
        print(f"\n{border}")
        print(title)
        print(border)
    
    @staticmethod
    def print_step_header(step_num: int, step_name: str) -> None:
        """ã‚¹ãƒ†ãƒƒãƒ—ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¡¨ç¤º"""
        print(f"\nStep {step_num}: {step_name}")
        print("-" * (10 + len(step_name)))
    
    @staticmethod
    def print_completion_message() -> None:
        """å®Ÿé¨“å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º"""
        print("\n" + "=" * 60)
        print("=== ALL EXPERIMENTS COMPLETED ===")
        print("To test different models:")
        print("  python test/test_models.py")
        print("For comprehensive analysis:")
        print("  python test/main_complete.py")
        print("=" * 60)


class ProgressReporter:
    """å®Ÿé¨“é€²è¡ŒçŠ¶æ³ã®å ±å‘Šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, total_experiments: int):
        """
        Args:
            total_experiments: ç·å®Ÿé¨“æ•°
        """
        self.total_experiments = total_experiments
        self.completed_experiments = 0
    
    def start_experiment(self, experiment_name: str) -> None:
        """å®Ÿé¨“é–‹å§‹ã‚’å ±å‘Š"""
        self.completed_experiments += 1
        progress = (self.completed_experiments / self.total_experiments) * 100
        
        print(f"\n[{self.completed_experiments}/{self.total_experiments}] ({progress:.1f}%) {experiment_name}")
    
    def complete_experiment(self, experiment_name: str) -> None:
        """å®Ÿé¨“å®Œäº†ã‚’å ±å‘Š"""
        print(f"âœ“ Completed: {experiment_name}")
    
    def final_summary(self) -> None:
        """æœ€çµ‚ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print(f"\nğŸ‰ All {self.total_experiments} experiments completed successfully!")


def format_metric_table(metrics_data: Dict[str, Dict[str, float]], 
                       experiment_names: List[str]) -> str:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if not metrics_data or not experiment_names:
        return "No data to format."
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’å–å¾—
    metric_names = list(next(iter(metrics_data.values())).keys())
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
    header = f"{'Experiment':<20}"
    for metric in metric_names:
        header += f" {metric.upper():<12}"
    
    lines = [header, "-" * len(header)]
    
    # ãƒ‡ãƒ¼ã‚¿è¡Œä½œæˆ
    for exp_name in experiment_names:
        if exp_name in metrics_data:
            line = f"{exp_name:<20}"
            for metric in metric_names:
                value = metrics_data[exp_name].get(metric, 0.0)
                line += f" {value:<12.4f}"
            lines.append(line)
    
    return "\n".join(lines)
