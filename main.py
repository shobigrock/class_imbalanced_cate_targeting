"""
CATEæ¨å®šå®Ÿé¨“ã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å®Ÿé¨“ã®å®Ÿè¡Œåˆ¶å¾¡ã®ã¿ã‚’è¡Œã„ã€
å…·ä½“çš„ãªå®Ÿè£…ã¯ shared/ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«å§”è­²ã—ã¾ã™ã€‚
"""

from shared.experiment_config import get_config, create_custom_config, PRESET_CONFIGS
from shared.experiment_runner import ExperimentRunner, ComparisonExperiment
from shared.result_formatter import ResultFormatter, ProgressReporter


def run_basic_experiments():
    """åŸºæœ¬çš„ãªå®Ÿé¨“ã‚»ãƒƒãƒˆã‚’å®Ÿè¡Œ"""
    ResultFormatter.print_experiment_header("BASIC EXPERIMENTS")
    
    # åŸºæœ¬å®Ÿé¨“ã®è¨­å®š
    basic_experiments = ["basic_linear", "basic_logistic", "basic_tree"]
    
    progress = ProgressReporter(len(basic_experiments))
    results = []
    
    for preset_name in basic_experiments:
        progress.start_experiment(f"{preset_name.replace('_', ' ').title()} Experiment")
        
        # è¨­å®šå–å¾—ã¨å®Ÿé¨“å®Ÿè¡Œ
        config = get_config(preset_name)
        runner = ExperimentRunner(config)
        result = runner.run_single_experiment()
        
        # çµæœè¡¨ç¤º
        ResultFormatter.print_experiment_summary(result)
        results.append(result)
        
        progress.complete_experiment(preset_name)
    
    return results


def run_undersampling_comparison():
    """ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿè¡Œ"""
    ResultFormatter.print_experiment_header("UNDERSAMPLING COMPARISON EXPERIMENT")
    
    # æ¯”è¼ƒå®Ÿé¨“ç”¨ã®è¨­å®š
    config = get_config("undersampling_comparison")
    
    # æ¯”è¼ƒå®Ÿé¨“å®Ÿè¡Œ
    comparison = ComparisonExperiment(config)
    comparison_results = comparison.run_undersampling_comparison()
    
    # çµæœè¡¨ç¤º
    ResultFormatter.print_comparison_results(comparison_results)
    
    return comparison_results


def run_custom_experiment():
    """ã‚«ã‚¹ã‚¿ãƒ å®Ÿé¨“ã®ä¾‹"""
    ResultFormatter.print_experiment_header("CUSTOM EXPERIMENT EXAMPLE")
    
    # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šä½œæˆ
    custom_config = create_custom_config(
        n_samples=4000,
        reward_type="logistic",
        model_type="s_learner",
        use_undersampling=True,
        k_factor=2.5
    )
    
    # å®Ÿé¨“å®Ÿè¡Œ
    runner = ExperimentRunner(custom_config)
    result = runner.run_single_experiment()
    
    # çµæœè¡¨ç¤º
    ResultFormatter.print_experiment_summary(result)
    ResultFormatter.print_detailed_evaluation(result["evaluation"])
    
    return result


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ Starting CATE Estimation Experiments")
    print("ğŸ“Š Imbalanced Data Undersampling Analysis")
    
    # åˆ©ç”¨å¯èƒ½ãªè¨­å®šã‚’è¡¨ç¤º
    print(f"\nğŸ“‹ Available preset configurations: {', '.join(PRESET_CONFIGS.keys())}")
    
    try:
        # 1. åŸºæœ¬å®Ÿé¨“
        basic_results = run_basic_experiments()
        
        # 2. ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”è¼ƒå®Ÿé¨“
        comparison_result = run_undersampling_comparison()
        
        # 3. ã‚«ã‚¹ã‚¿ãƒ å®Ÿé¨“ä¾‹
        custom_result = run_custom_experiment()
        
        # æœ€çµ‚ã‚µãƒãƒªãƒ¼
        print("\n" + "=" * 80)
        print("ğŸ“ˆ EXPERIMENT SUMMARY")
        summary = ResultFormatter.create_results_summary(basic_results + [custom_result])
        print(summary)
        
        # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        ResultFormatter.print_completion_message()
        
    except KeyboardInterrupt:
        print("\nâš ï¸  Experiment interrupted by user")
    except Exception as e:
        print(f"\nâŒ Experiment failed with error: {e}")
        raise


if __name__ == '__main__':
    main()