"""
CATEæ¨å®šå®Ÿé¨“ã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å®Ÿé¨“ã®å®Ÿè¡Œåˆ¶å¾¡ã®ã¿ã‚’è¡Œã„ã€
å…·ä½“çš„ãªå®Ÿè£…ã¯ shared/ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«å§”è­²ã—ã¾ã™ã€‚
"""

import sys
from typing import List, Dict, Any, Optional
from shared.cli_parser import CLIParser, CLIConfig
from shared.experiment_config import (
    get_config, create_custom_config, create_config_from_cli, 
    PRESET_CONFIGS, ExperimentConfig
)
from shared.experiment_runner import ExperimentRunner, ComparisonExperiment
from shared.result_formatter import ResultFormatter, ProgressReporter


def run_basic_experiments():
    """åŸºæœ¬çš„ãªå®Ÿé¨“ã‚»ãƒƒãƒˆã‚’å®Ÿè¡Œ"""
    ResultFormatter.print_experiment_header("BASIC EXPERIMENTS")
    
    # åŸºæœ¬å®Ÿé¨“ã®è¨­å®š
    basic_experiments = ["basic_linear", "basic_logistic", "basic_tree"]
    
    progress = ProgressReporter(len(basic_experiments))
    results = []
    
    for preset_name in basic_experiments:
        progress.start_experiment(f"{preset_name.replace('_', ' ').title()} Experiment")
        
        # è¨­å®šå–å¾—ã¨å®Ÿé¨“å®Ÿè¡Œ
        config = get_config(preset_name)
        runner = ExperimentRunner(config)
        result = runner.run_single_experiment()
        
        # çµæœè¡¨ç¤º
        ResultFormatter.print_experiment_summary(result)
        results.append(result)
        
        progress.complete_experiment(preset_name)
    
    return results


def run_undersampling_comparison():
    """ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿè¡Œ"""
    ResultFormatter.print_experiment_header("UNDERSAMPLING COMPARISON EXPERIMENT")
    
    # æ¯”è¼ƒå®Ÿé¨“ç”¨ã®è¨­å®š
    config = get_config("undersampling_comparison")
    
    # æ¯”è¼ƒå®Ÿé¨“å®Ÿè¡Œ
    comparison = ComparisonExperiment(config)
    comparison_results = comparison.run_undersampling_comparison()
    
    # çµæœè¡¨ç¤º
    ResultFormatter.print_comparison_results(comparison_results)
    
    return comparison_results


def run_custom_experiment():
    """ã‚«ã‚¹ã‚¿ãƒ å®Ÿé¨“ã®ä¾‹"""
    ResultFormatter.print_experiment_header("CUSTOM EXPERIMENT EXAMPLE")
    
    # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šä½œæˆ
    custom_config = create_custom_config(
        n_samples=4000,
        reward_type="logistic",
        model_type="s_learner",
        use_undersampling=True,
        k_factor=2.5
    )
    
    # å®Ÿé¨“å®Ÿè¡Œ
    runner = ExperimentRunner(custom_config)
    result = runner.run_single_experiment()
    
    # çµæœè¡¨ç¤º
    ResultFormatter.print_experiment_summary(result)
    ResultFormatter.print_detailed_evaluation(result["evaluation"])
    
    return result


def run_cli_experiments(cli_config: CLIConfig) -> List[Dict[str, Any]]:
    """CLIã‚³ãƒ³ãƒ•ã‚£ã‚°ã«åŸºã¥ã„ã¦å®Ÿé¨“ã‚’å®Ÿè¡Œ"""
    print(f"ğŸš€ Running CLI Experiments")
    print(f"ğŸ“Š Data source: {cli_config.data_source}")
    print(f"ğŸ”§ CATE methods: {', '.join(cli_config.cate_methods)}")
    print(f"ğŸ“ˆ K values: {cli_config.k_values}")
    print(f"ğŸ¯ QINI enabled: {cli_config.enable_qini}")
    
    # CLIã‚³ãƒ³ãƒ•ã‚£ã‚°ã‹ã‚‰å®Ÿé¨“è¨­å®šã‚’ä½œæˆ
    configs = create_config_from_cli(cli_config)
    total_experiments = len(configs)
    
    progress = ProgressReporter(total_experiments)
    results = []
    
    for i, config in enumerate(configs):
        exp_name = f"{config.model_type}_k{config.k_factor if config.use_undersampling else 'none'}"
        progress.start_experiment(f"Experiment {i+1}/{total_experiments}: {exp_name}")
        
        # å®Ÿé¨“å®Ÿè¡Œ
        runner = ExperimentRunner(config)
        result = runner.run_single_experiment()
        
        # è©³ç´°ãƒ­ã‚°ï¼ˆverboseæ™‚ã®ã¿ï¼‰
        if cli_config.verbose:
            ResultFormatter.print_experiment_summary(result)
        
        results.append(result)
        progress.complete_experiment(exp_name)
    
    return results


def save_results_to_file(results: List[Dict[str, Any]], output_file: str):
    """çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    import pandas as pd
    
    # çµæœãƒ‡ãƒ¼ã‚¿ã‚’å¹³å¦åŒ–
    flattened_data = []
    for result in results:
        row = {
            'model_type': result['config']['model_type'],
            'use_undersampling': result['config']['use_undersampling'],
            'k_factor': result['config'].get('k_factor', 0),
            'data_source': result['config'].get('data_source', 'synthetic'),
            'n_samples': result['config']['n_samples'],
            'test_size': result['config']['test_size'],
            'rmse': result['evaluation']['rmse'],
            'bias': result['evaluation']['bias'],
            'r2_score': result['evaluation'].get('r2_score', 0.0),
            'runtime': result['runtime']
        }
        
        # QINIä¿‚æ•°ãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
        if 'qini_coefficient' in result['evaluation']:
            row['qini_coefficient'] = result['evaluation']['qini_coefficient']
        
        flattened_data.append(row)
    
    # DataFrameä½œæˆã¨CSVä¿å­˜
    df = pd.DataFrame(flattened_data)
    df.to_csv(output_file, index=False)
    print(f"ğŸ’¾ Results saved to {output_file}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ CATE Estimation Experiments with Class Imbalance")
    print("ğŸ“Š Command-line Interface for Flexible Experimentation")
    
    # CLIå¼•æ•°è§£æ
    parser = CLIParser()
    cli_config = parser.parse_args()
    
    if cli_config is None:
        # ãƒ˜ãƒ«ãƒ—ãŒè¡¨ç¤ºã•ã‚ŒãŸã‹ã€å¼•æ•°ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ãŸå ´åˆ
        return
    
    try:
        # CLIæŒ‡å®šã®å®Ÿé¨“ã‚’å®Ÿè¡Œ
        results = run_cli_experiments(cli_config)
        
        # çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        print("\n" + "=" * 80)
        print("ğŸ“ˆ EXPERIMENT RESULTS SUMMARY")
        summary = ResultFormatter.create_results_summary(results)
        print(summary)
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆæŒ‡å®šã•ã‚ŒãŸå ´åˆï¼‰
        if cli_config.output_file:
            save_results_to_file(results, cli_config.output_file)
        
        # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        ResultFormatter.print_completion_message()
        
    except KeyboardInterrupt:
        print("\nâš ï¸  Experiment interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Experiment failed with error: {e}")
        if cli_config.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


def run_preset_experiments():
    """ãƒ—ãƒªã‚»ãƒƒãƒˆå®Ÿé¨“ã‚’å®Ÿè¡Œï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰"""
    print("ğŸš€ Running Preset Experiments")
    print("ğŸ“‹ Use 'python main.py --help' for custom CLI options")
    
    try:
        # 1. åŸºæœ¬å®Ÿé¨“
        basic_results = run_basic_experiments()
        
        # 2. ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”è¼ƒå®Ÿé¨“
        comparison_result = run_undersampling_comparison()
        
        # 3. ã‚«ã‚¹ã‚¿ãƒ å®Ÿé¨“ä¾‹
        custom_result = run_custom_experiment()
        
        # æœ€çµ‚ã‚µãƒãƒªãƒ¼
        print("\n" + "=" * 80)
        print("ğŸ“ˆ EXPERIMENT SUMMARY")
        summary = ResultFormatter.create_results_summary(basic_results + [custom_result])
        print(summary)
        
        # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        ResultFormatter.print_completion_message()
        
    except KeyboardInterrupt:
        print("\nâš ï¸  Experiment interrupted by user")
    except Exception as e:
        print(f"\nâŒ Experiment failed with error: {e}")
        raise


if __name__ == '__main__':
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if len(sys.argv) > 1:
        # CLIå¼•æ•°ãŒæä¾›ã•ã‚ŒãŸå ´åˆã¯CLIå®Ÿé¨“ã‚’å®Ÿè¡Œ
        main()
    else:
        # å¼•æ•°ãŒãªã„å ´åˆã¯ãƒ—ãƒªã‚»ãƒƒãƒˆå®Ÿé¨“ã‚’å®Ÿè¡Œï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
        print("ğŸ“‹ No CLI arguments provided. Running preset experiments...")
        print("ğŸ’¡ Use 'python main.py --help' for CLI options")
        run_preset_experiments()